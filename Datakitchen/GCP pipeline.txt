import subprocess
from google.cloud import storage

# Set the MySQL database credentials
db_user = "root"
db_password = "Aditya908"
db_host = "zeyodb.cz7qhc39aphp.ap-south-1.rds.amazonaws.com"
db_name = "zeyodb"
table_name = "salesdata"

# Use the mysqldump command to export the MySQL data to a file
dump_command = f"mysqldump -u {db_user} -p{db_password} -h {db_host} {db_name} {table_name} > {table_name}.sql"
subprocess.run(dump_command, shell=True, check=True)

# Upload the file to the GCP Storage bucket
storage_client = storage.Client()
bucket_name = "your-bucket-name"
bucket = storage_client.bucket(bucket_name)
blob = bucket.blob(f"{table_name}.sql")
blob.upload_from_filename(f"{table_name}.sql")

---------------------------------------------------------------------------------------------------------
from dkutils.validation import validate_globals

try:

success == False

forecasted_sales_len = -1

min_forecast_sales = None

max_forecast_sales = None

validate_globals ( [

])

'MODEL_FILENAME',

'DATA_FOR_FORECASTING_FILENAME',

'FEATURES',

'FORECASTED_SALES_FILENAME'

#Load the model

model =

joblib.load(f'docker-share/{MODEL_FILENAME}')

# Load the data and predict sales

all_data_df = pd. read_csv (f'docker-share/{DATA_FOR_FORECASTING_FILENAME}')

feature_data_df = all_data_df.loc[:, FEATURES]

forecasted_sales = model.predict (feature_data_df)

min_forecast sales = forecasted_sales.min()

max_forecast_sales = forecasted_sales.max ( )

forecasted_sales_len = len (forecasted_sales)

all_data_df.insert (4, 'forecasted_weekly_sales', forecasted_sales.tolist())

all_data_df.to_csv (f'docker-share/{FORECASTED_SALES_FILENAME}', index=False)

success = True

except Exception as e:

LOGGER.error (f'Model forecasting failed: \n{traceback. format_exc()}')

---------------------------------------------------------------------------------------------------------


