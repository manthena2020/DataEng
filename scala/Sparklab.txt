===================================================================================================================
Prining  the input
===================================================================================================================
package pack1

object obj {
  def main(args: Array[String]): Unit = {
    println("=============== started ===============")
    val a = 2
    println(a)
    val b = "zeyo"
    println(b)
    
  }
}
===================================================================================================================
Printing  the input
===================================================================================================================
package pack1

object obj {
  def main(args: Array[String]): Unit = {
    println("=============== started ===============")
    val a = 2
    println(a)
    val b = "zeyo"
    println(b)
    val lis = List(1, 2, 3, 4)
    println(lis)
    
  }
}
===================================================================================================================
Printing the input
===================================================================================================================
package pack1

object obj {
  def main(args: Array[String]): Unit = {
    println("=============== started ===============")
    val a = 2
    println(a)
    val b = "zeyo"
    println(b)
    val lis = List(1, 2, 3, 4)
    println(lis)
    println("====== raw list ===============")
    lis.foreach(println)    
  }
}
===================================================================================================================
Filter operations x => x > 2 filtering greater than values
===================================================================================================================
 pack1

object obj {
  def main(args: Array[String]): Unit = {
    println("=============== started ===============")
    val a = 2
    println(a)
    val b = "zeyo"
    println(b)
    val lis = List(1, 2, 3, 4)
    println(lis)
    println("====== raw list ===============")
    lis.foreach(println)
    val plis = lis.filter(x => x > 2)
    println("====== processed list ===============")
    plis.foreach(println)
  }
}
2
zeyo
List(1, 2, 3, 4)
====== raw list ===============
1
2
3
4
====== processed list ===============
3
4

===================================================================================================================

import org.apache.spark.SparkContext //rdd
import org.apache.spark.sql.SparkSession // dataframe
import org.apache.spark.SparkConf
===================================================================================================================
package spark pack

import org.apache.spark._
import sys.process._



object sparkobj {


	def main(args:Array[String]):Unit={

	  
	    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
	    val sc = new SparkContext(conf)
	    sc.setLogLevel("ERROR")
	    

			val data = sc.textFile("file:///home/itv004875/data/txns")
					val gymdata= data.filter(x=>x.contains("Gymnastics"))
					"hadoop fs -rmr /user/itv004875/spark/datagym".!
					gymdata.saveAsTextFile("/user/itv004875/spark/datagym")
					println("==============done =================")

	}
}
===================================================================================================================
SPARK_SHELL:
 spark-submit --class sparkpack.sparkobj SparkFirst-0.0.1-SNAPSHOT.jar
=================================================================================================================== 
 MaP operations- x => x*2 Multiplication
 ===================================================================================================================
 object obj2 {
   def main(args:Array[String]): Unit ={
    println("==========OPERATION STARTED================")
    val list = List(1,2,3,4)
    println(list)
    println("==========RAW LIST========================")
    list.foreach(println)
    val plist = list.map(x => x*2)
    println("==========PROCESSED LIST=================")
    plist.foreach(println)
    
}
}
==========OPERATION STARTED================
List(1, 2, 3, 4)
==========RAW LIST========================
1
2
3
4
==========PROCESSED LIST=================
2
4
6
8

===================================================================================================================
Map operations-(Dividing the values and printing with out decimals)
===================================================================================================================
package pack1

object obj3 {
  def main(args:Array[String]): Unit ={
    println("==========OPERATION STARTED================")
    val list = List(1,2,3,4)
    println(list)
    println("==========RAW LIST========================")
    list.foreach(println)
    val plist = list.map(x => x/2)
    println("==========PROCESSED LIST=================")
    plist.foreach(println)
    
}
}
==========OPERATION STARTED================
List(1, 2, 3, 4)
==========RAW LIST========================
1
2
3
4
==========PROCESSED LIST=================
0
1
1
2
===================================================================================================================
Map operations-(Dividing the values and printing with decimals)
===================================================================================================================
package pack1

object obj4 {
  def main(args:Array[String]): Unit ={
    println("==========OPERATION STARTED================")
    val list = List(1,2,3,4)
    println(list)
    println("==========RAW LIST========================")
    list.foreach(println)
    val plist = list.map(x => x.toDouble/2)
    println("==========PROCESSED LIST=================")
    plist.foreach(println)
    
  }
}

x.toDouble/2 (which divides the element by 2 and converts it to a Double,
a double is a data type that is used to represent a decimal number)
==========OPERATION STARTED================
List(1, 2, 3, 4)
==========RAW LIST========================
1
2
3
4
==========PROCESSED LIST=================
0.5
1.0
1.5
2.0

====================================================================================================================
Filtering x=> contains( word that matches may not be excat)
====================================================================================================================
package pack1
object obj5 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Zeyobron","analytics","Zeyo")
   list.foreach(println)
   val plist = list.filter(x => x.contains("Zeyo"))
   println("==============FOUND WORDS WITH ZEYO=================")
   plist.foreach(println)
  

}
}
===============INPUT==============
Zeyobro
analytics
Zeyo
==============FOUND WORDS WITH ZEYO=================
Zeyobro
Zeyo
===================================================================================================================
Filtering x=>Equals ( finding the word )
===================================================================================================================
package pack1
object obj6 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Zeyobron","analytics","Zeyo")
   list.foreach(println)
   val plist = list.filter(x => x.equals("Zeyo"))
   println("==============FOUND WORDS WITH ZEYO=================")
   plist.foreach(println)
  

}
}
===============INPUT==============
Zeyobron
analytics
Zeyo
==============FOUND WORDS WITH ZEYO=================
Zeyo

===================================================================================================================
Filter and map operation X.>contains & x.toLowerCase (Filtering word with case sensitive 
 and  Mapping converting the filtered data to  lower case)
===================================================================================================================
package pack1
object obj7 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Zeyobron","analytics","Zeyo")
   list.foreach(println)
   val plist = list.filter(x => x.toLowerCase.contains("zeyo"))
                   .map(x => x.toLowerCase)
               
   println("==============FOUND WORDS WITH ZEYO=================")
   plist.foreach(println)
  

}
}
===============INPUT==============
Zeyobron
analytics
Zeyo
==============FOUND WORDS WITH ZEYO=================
zeyobron
zeyo

===================================================================================================================
MAp operations - (Concating string zeyo to input data as per code)
===================================================================================================================
package pack1
object obj8 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Zeyobron","analytics","Zeyo")
   list.foreach(println)
   val plist = list.map(x => x.concat("-zeyo"))
                   
               
   println("==============CONCATE OPERATION=================")
   plist.foreach(println)
  

}
}
===============INPUT==============
Zeyobron
analytics
Zeyo
==============CONCATE OPERATION=================
Zeyobron-zeyo
analytics-zeyo
Zeyo-zeyo

===================================================================================================================
Map operation 
===================================================================================================================
package pack1
object obj9 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Zeyobron","Zeyotics","Zeyo")
   list.foreach(println)
   val plist = list.map(x => x.replace("Zeyo","tera"))
                   
               
   println("==============CONCATE OPERATION=================")
   plist.foreach(println)
 
}
}
===================================================================================================================
Map operations -x.replace  Replace opeartions
===================================================================================================================
package pack1
object obj9 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Zeyobron","Zeyotics","Zeyo")
   list.foreach(println)
   val plist = list.map(x => x.replace("Zeyo","tera")) //replace operation
                   
               
   println("==============CONCATE OPERATION=================")
   plist.foreach(println)
  

}
}
===============INPUT==============
Zeyobron
Zeyotics
Zeyo
==============REPLACE OPERATION=================
terabron
teratics
tera

===================================================================================================================
Flat Map opeartions - x => x.split("~") Spliting the data with delimiter
===================================================================================================================
package pack1
object obj10 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("A~B","C~D","E~F")
   list.foreach(println)
   val plist = list.flatMap(x => x.split("~")) //flaten the element delimiter ~
                   
               
   println("==============FLATEN OPERATION=================")
   plist.foreach(println)
  

}
}
===============INPUT==============
A~B
C~D
E~F
==============FLATEN OPERATION=================
A
B
C
D
E
F

===================================================================================================================
Filter(contains)-flatmap(delimiter)-map(replace)-map(concat)
(x => x.contains("India"))-(x => x.split("-")-(x => x.replace("India","local"))-(x => x.concat("-i love u "))
===================================================================================================================
package pack1
object obj11 {
  def main(args:Array[String]): Unit ={
   println("===============INPUT==============")
   val list = List("Amazon-jeff-America","microsoft-Billgates-America","TCS-Tata-India","Reliance-ambani-India")
   list.foreach(println)
   val flist = list.filter(x => x.contains("India"))
      println("==============SEARCH OPERATION=================")   
   flist.foreach(println)
   val plist = flist.flatMap(x => x.split("-")) //flaten the element delimiter ~
                 
  println("==============FLATEN OPERATION=================")
  plist.foreach(println)
  val rpl = plist.map(x => x.replace("India","local"))
      
  println("==============REPLACE OPERATION=================")
  rpl.foreach(println)
  
  val con = rpl.map(x => x.concat("-i love u "))
  con.foreach(println)
}
}
===============INPUT==============
Amazon-jeff-America
microsoft-Billgates-America
TCS-Tata-India
Reliance-ambani-India
==============SEARCH OPERATION=================
TCS-Tata-India
Reliance-ambani-India
==============FLATEN OPERATION=================
TCS
Tata
India
Reliance
ambani
India
==============REPLACE OPERATION=================
TCS
Tata
local
Reliance
ambani
local
==============CONCAT OPERATION=================
TCS-i love u 
Tata-i love u 
local-i love u 
Reliance-i love u 
ambani-i love u 
local-i love u 

===================================================================================================================
package pack1
object obj12 {
  def main(args:Array[String]): Unit ={
    println("==========================INPUT======================")
    val list = List("State->Tamilnadu~city->chennai","State->Karnataka~city->Banglore","State->Telangana~city->Hyderabad")
    println(list)
        println("==========================STAGE-1======================")
    list.foreach(println)
    
    val state = list.flatMap(x => x.split("~"))
    println("==========================STAGE-2======================")
    state.foreach(println)
    val lstate = state.filter(x => x.contains("State"))
    println("==========================STAGE-3======================")
    lstate.foreach(println)
    val lcity = state.filter(x => x.contains("city"))
    println("==========================STAGE-4======================")
    lcity.foreach(println)
    val llstate = lstate.map(x => x.replace("State->"," "))
    println("==========================STATE======================")
    llstate.foreach(println)
    val llcity = lcity.map(x => x.replace("city->"," "))
    println("==========================CITY======================")
    llcity.foreach(println)
    println("========================== END======================")
  }
}
===================================================================================================================
package pack2

import org.apache.spark.SparkContext 
import org.apache.spark.SparkConf


object sparkobj {
  
  def main(args:Array[String]):Unit={

	  	    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
	  	    val sc = new SparkContext(conf)
	  	    sc.setLogLevel("ERROR")
	  	    
	  	    val data = sc.textFile("file:///E:/softwares/Dataengineering/Spark/data1")
	  	    println("===================RAW RDD====================================")
	  	    data.foreach(println)
	  	    
   val state = data.flatMap(x => x.split("~"))
    println("==========================STAGE-2======================")
    state.foreach(println)
    val lstate = state.filter(x => x.contains("State"))
    println("==========================STAGE-3======================")
    lstate.foreach(println)
    val lcity = state.filter(x => x.contains("city"))
    println("==========================STAGE-4======================")
    lcity.foreach(println)
    val llstate = lstate.map(x => x.replace("State->"," "))
    println("==========================STATE======================")
    llstate.foreach(println)
    val llcity = lcity.map(x => x.replace("city->"," "))
    println("==========================CITY======================")
    llcity.foreach(println)
    println("========================== END======================")
    
    llstate.coalesce(1).saveAsTextFile("file:///E:/softwares/Dataengineering/Spark/data2")
    
}
}
===================================================================================================================
object obj13 {
  def main(args:Array[String]): Unit ={
    println("==========================INPUT======================")
    val list = List("BigData-Spark-Hive","Spark-Hadoop-Hive","Sqoop-Hive-Spark","Sqoop-BD-Hive")
     println(list)
        println("==========================STAGE-1======================")
    list.foreach(println)
    
    val state = list.flatMap(x => x.split("-")).distinct
    println("==========================STAGE-2======================")
    state.foreach(println)
    val lstate = state.map(x => x.replace("BD","BigData"))
    println("==========================STAGE-3======================")
    lstate.foreach(println)
    val lcity = state.map(x => ("Tech->")+x.concat("Trainer->Sai"))
    println("==========================STAGE-4======================")
    lcity.foreach(println)
   
  }
}
===================================================================================================================

package pack2

import org.apache.spark.SparkContext 
import org.apache.spark.SparkConf
object sparkobj1 {
  
  def main(args:Array[String]):Unit={

	  	    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
	  	    val sc = new SparkContext(conf)
	  	    sc.setLogLevel("ERROR")
	  	    
	  	    val data = sc.textFile("file:///E:/softwares/Dataengineering/Sqoop/salestable1/part-m*")
	  	    println("===================RAW RDD====================================")
	  	    data.take(10).foreach(println)
	  	    
   val state = data.filter( x => x.length() >108)
    println("==========================lenght>100======================")
    state.foreach(println)
    val lstate = state.flatMap(x => x.split(","))
    println("==========================FLATTEN DATA======================")
    lstate.foreach(println)
    
    val lcity = lstate.map(x => x.replace("-",""))
    println("==========================Replaced DATA WITH-======================")
    lcity.foreach(println)
    val llstate = lcity.map(x => x + ",ZEYO")
    println("========================== ZEYO CONCATED======================")
    llstate.foreach(println)
        
    llstate.coalesce(1).saveAsTextFile("file:///E:/softwares/Dataengineering/Spark/data3")
     println("==========================TASK COMPLETED======================")
    
}
}


===================================================================================================================
SCHEMA RDD
===================================================================================================================
object sparkobj2 {

case class zschema(id:String,product:String,Category:String,mode:String)

def main(args:Array[String]):Unit={

		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val data = sc.textFile("file:///E:/softwares/Dataengineering/spark/datatxns")
				println("===================RAW RDD====================================")
				println
				data.foreach(println)
				println
				val mapsplit = data.map( x => x.split(","))
				mapsplit.foreach(println)
				    
				val srdd = mapsplit.map(x => zschema(x(0),x(1),x(2),x(3)))
				val filter = srdd.filter( x => x.Category.contains("Gymnastics") )
				println("===================COLOUMN GYM DATA ====================================")
				filter.foreach(println)
				println("==========================TASK COMPLETED======================")

}
}
===================================================================================================================

object sparkobj3 {

case class zschema(id:String,product:String,Category:String,mode:String)

def main(args:Array[String]):Unit={

		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
				import spark.implicits._

				val data =sc.textFile("file:///E:/softwares/Dataengineering/spark/datatxns")
				println("===================RAW RDD====================================")
				data.foreach(println)

				val row = data.filter(x => x.contains("Gymnastics"))
				println("===================ROW DATA====================================")
				row.foreach(println)   

				val mapsplit = data.map( x => x.split(","))


				val srdd = mapsplit.map(x => zschema(x(0),x(1),x(2),x(3)))
				val filter = srdd.filter( x => x.Category.contains("Gymnastics") && x.id.toInt>14)


				println("===================COLOUMN GYM DATA ====================================")
				filter.foreach(println)
				println("==========================TASK COMPLETED======================")

				println("=================== DATA FRAME ====================================")
				println()
				val df = filter.toDF()

				df.show()
				df.write.parquet("file:///E:/softwares/Dataengineering/Spark/Parquet")
				println("=================== TASK COMPLETED ====================================")
}
}



===================================================================================================================
ROW RDD
===================================================================================================================
object sparkobj4 {


def main(args:Array[String]):Unit={

		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
				import spark.implicits._

				val data =sc.textFile("file:///E:/softwares/Dataengineering/spark/datatxns")
				println("===================RAW RDD====================================")
				data.foreach(println)
				
				val mapsplit = data.map( x => x.split(","))
        val rowrdd = mapsplit.map( x => Row(x(0),x(1),x(2),x(3)))
        
				val filterdata = rowrdd.filter( x => 
				                                      x(2).toString().contains("Gymnastics") &&
				                                      x(0).toString().toInt >20)
				                       				  
				println("===================ROW DATA====================================")
			  filterdata.foreach(println)   

			val rowschema = StructType(Array(
			    StructField("id",StringType),
			    StructField("category",StringType),
			    StructField("product",StringType),
			    StructField("mode",StringType)))
			    
			    val df = spark.createDataFrame( filterdata, rowschema)
			    df.show()

	
				df.show()
				df.write.parquet("file:///E:/softwares/Dataengineering/Spark/Parquet1")
				
				println("=================== TASK COMPLETED ====================================")
}
}
===================================================================================================================
DATA FRAME
===================================================================================================================

object sparkobj6 {


def main(args:Array[String]):Unit={

		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
				import spark.implicits._

				val df = spark.read.format("csv").load("file:///E:/softwares/Dataengineering/spark/datatxns")
			    df.show()
			    
			    df.createOrReplaceTempView("tab")
			    val finaldf = spark.sql("select * from tab where _c1='Gymnastics'")
			    finaldf.show()

				
				
}
				
				
		
        
			
}
===================================================================================================================
object sparkobj6 {


def main(args:Array[String]):Unit={

		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
				import spark.implicits._

				val df = spark.read.format("csv").load("file:///E:/softwares/Dataengineering/spark/datatxns")
			    df.show()
			    
			    df.createOrReplaceTempView("tab")
			    val finaldf = spark.sql("select * from tab where _c1='Gymnastics'")
			    finaldf.show()

				finaldf.write.parquet("file:///E:/softwares/Dataengineering/Spark/Parquet2")
				
				println("=================== TASK COMPLETED ====================================")
				
}
				
				
		
        
			
}


===================================================================================================================
Header
===================================================================================================================
object sparkobj7 {


def main(args:Array[String]):Unit={

		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
				import spark.implicits._

				val df = spark
				               .read
				               .format("csv")
				               .option("header","true")
				               .load("file:///E:/softwares/Dataengineering/spark/datatxns")
			    df.show()
			    
			   
			    df.createOrReplaceTempView("tab")
			    val finaldf = spark.sql("select * from tab where Gymnastics1='Jumping'")
			    finaldf.show()

				/*finaldf.write.parquet("file:///E:/softwares/Dataengineering/Spark/Parquet2")
				
				println("=================== TASK COMPLETED ====================================")*/
				
}
===================================================================================================================
DSL

===================================================================================================================
object sparkobj8 {


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val df = spark
					.read
					.format("csv")
					.option("header","true")
					.load("file:///E:/softwares/Dataengineering/spark/datatxns")
					df.show()

					val finaldf = df.filter("Gymnastics1='Jumping'")
					
					
					

					/*df.createOrReplaceTempView("tab")
					val finaldf = spark.sql("select * from tab where Gymnastics1='Jumping'")
					finaldf.show()

					finaldf.write.parquet("file:///E:/softwares/Dataengineering/Spark/Parquet2")

				println("=================== TASK COMPLETED ====================================")*/

	}
	
===================================================================================================================
// delimiter//
===================================================================================================================
object sparkobj9 {  // delimiter//


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val df = spark
					.read
					.format("csv")
					.option("header","true")
					.option("delimiter","#")
					.load("file:///E:/softwares/Dataengineering/spark/datatxns1")
					df.show()

					/*val finaldf = df.filter("Gymnastics1='Jumping'")
					
					
					

					df.createOrReplaceTempView("tab")
					val finaldf = spark.sql("select * from tab where Gymnastics1='Jumping'")
					finaldf.show()

					finaldf.write.parquet("file:///E:/softwares/Dataengineering/Spark/Parquet2")

				println("=================== TASK COMPLETED ====================================")*/

	}
}
===================================================================================================================
object sparkobj10 { // reading avro file


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val df = spark
					.read
					.format("avro")
					.option("header","true")
					.option("delimiter","#")
					.load("file:///E:/softwares/Dataengineering/spark/data.avro")
					df.show()
	
	
					
	/*val finaldf = df.filter("Gymnastics1='Jumping'")
					
					
					

					df.createOrReplaceTempView("tab")
					val finaldf = spark.sql("select * from tab where Gymnastics1='Jumping'")
					finaldf.show()

					finaldf.write.avro("file:///E:/softwares/Dataengineering/Spark/avro")

				println("=================== TASK COMPLETED ====================================")*/

	}
}
===================================================================================================================
READING TABLE FROM MYSQL
===================================================================================================================

object sparkobj11 { // reading file from RDBMS


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val sqldf = spark
					.read
					.format("jdbc")
					.option("url","jdbc:mysql://zeyodb.cyewo8l7hsyn.ap-northeast-1.rds.amazonaws.com:3306/itv004875")
					.option("driver","com.mysql.jdbc.Driver")
					.option("dbtable","custjob")
					.option("user","root")
					.option("password","Aditya908")
					.load()
					println()
					println()
					println("======================READING  DATA FROM MYSQL==================================")
					println()					
					sqldf.show()
					println()
					println("======================  END OF TASK ==================================")
					
					
					




	}
}
===================================================================================================================
READING FROM RDBMS AND WRITING TO LOCAL
===================================================================================================================
object sparkobj12 { // 


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val sqldf = spark
					.read
					.format("jdbc")
					.option("url","jdbc:mysql://zeyodb.cyewo8l7hsyn.ap-northeast-1.rds.amazonaws.com:3306/itv004875")
					.option("driver","com.mysql.jdbc.Driver")
					.option("dbtable","custjob")
					.option("user","root")
					.option("password","Aditya908")
					.load()
					println()
					println()
					println("======================READING  DATA FROM MYSQL==================================")
					println()					
					sqldf.show()

					val finaldf = sqldf.filter("location='chennai'")
					println()
					finaldf.show()

					finaldf
					.write
					.format("CSV")
					.option("header","true")
					.save("file:///E:/softwares/Dataengineering/Spark/mysql")



					println()
					println("======================  END OF TASK ==================================")







	}
}
===================================================================================================================
READING FROM RDBMS AND WRITING TO RDBMS
===================================================================================================================
object sparkobj13 { // reading file from RDBMS and writing to RDBMS


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val sqldf = spark
					.read
					.format("jdbc")
					.option("url","jdbc:mysql://zeyodb.cyewo8l7hsyn.ap-northeast-1.rds.amazonaws.com:3306/itv004875")
					.option("driver","com.mysql.jdbc.Driver")
					.option("dbtable","custjob")
					.option("user","root")
					.option("password","Aditya908")
					.load()
					println()
					println()
					println("======================READING  DATA FROM MYSQL==================================")
					println()					
					sqldf.show()

					val finaldf = sqldf.filter("location='chennai'")
					println()
					finaldf.show()

					finaldf
					.write
					.format("jdbc")
					.option("url","jdbc:mysql://zeyodb.cyewo8l7hsyn.ap-northeast-1.rds.amazonaws.com:3306/itv004875")
					.option("driver","com.mysql.jdbc.Driver")
					.option("dbtable","prrrr")
					.option("user","root")
					.option("password","Aditya908")					
					
					println()
					println("======================  END OF TASK ==================================")


	}
}
===================================================================================================================
import org.apache.spark.SparkContext 
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql._ 
import org.apache.spark.sql.SaveMode
===================================================================================================================
JSON FILE READ
===================================================================================================================

object sparkobj14 { // reading json file


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val df = spark
					.read
					.format("json")
					.load("file:///E:/softwares/Dataengineering/spark/products.json")
					println()
					println()
					println("======================json DATA==================================")
					println()
					df.show()
					
					val finaldf = df.filter("age>25")
					println()
					println()
					println("======================PROCESSED DATA==================================")
					println()
					finaldf.show()
					
					finaldf
					       .write
					       .format("CSV")
					       .mode("append")
					       .save("file:///E:/softwares/Dataengineering/spark/csv")
					       println("====================== DATA WRITTEN SUCESSFULLY ==================================")
					       





	}
}
===================================================================================================================
XML READS
===================================================================================================================
object sparkobj15 { // reading xml file


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					val df = spark
					.read
					.format("xml")
					.option("rowtag","book")
					.load("file:///E:/softwares/Dataengineering/spark/book.xml")
					println()
					println()
					println("======================XML DATA==================================")
					println()
					df.show()
					
					





	}
}
===================================================================================================================
object sparkobj2  { /* displaying zero first */
	def main(args: Array[String]): Unit = {

			val input = List(1,0,0,0,1,1)
					val zerofirst = input.partition(_== 0)
					val output = zerofirst._1 ::: zerofirst._2

					output.foreach(println)


	}
}
/*The given code is a Scala program that defines an object named "sparkobj2".
The object has a single method named "main" that takes an array of Strings 
as an input parameter. 
The main method defines a List named "input"
containing six integers, 
and then performs a partition operation
on the list based on the value of each element (0 or 1).


The partition method returns a tuple containing two lists: 

the first list contains all the elements that satisfy the given predicate
(in this case, the value is 0),
 and the second list contains all the elements that do not satisfy the given predicate (in this case, the value 
    is 1).

The program then concatenates these two lists using the ":::" operator
to create a new list named "output". 
Finally, the program uses a foreach loop to print each element of the 
"output" list.
 */

object sparkobj3  { /* displaying zero first */
	def main(args: Array[String]): Unit = {

			val input = List(1, 0, 0, 0, 1, 1)
					val zeros = input.filter(_ == 0)
					val nonZeros = input.filter(_ != 0)
					val output = zeros ++ nonZeros
					output.foreach(println)

	}===================================================================================================================
}



object sparkobj4 {// to find out email doesnt have @ string 


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._
					val df = spark
					.read
					.format("csv")
					.option("header", "false")
					.option("delimiter", ",")
					.load("file:///c:/data/email.txt")
					.toDF("sno", "name", "email","phone no")

					println("-----------------DATA--------------")
					df.show()

					/*val filteredDF = df.filter(!$"email".contains("@"))
					println("----------------- DATA MISSIN @ --------------")
					filteredDF.show()
					 */
					val filteredDF = df.filter(length(trim(col("phone no").cast(StringType))) < 10)

					
					println("----------------- DATA MISSIN @ --------------")
					filteredDF.show()




	}
}
===================================================================================================================
object sparkobj5 {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

/*val table1 = Seq(1, 2, 3).toDF("id").withColumn("dummy",monotonicallyIncreasingId()+1)
val table2 = Seq("A", "B").toDF("name").withColumn("dummy",monotonicallyIncreasingId())
                                                 
table1.show()
table2.show()*/
					
					val table1 = Seq(1, 2, 3).toDF("id").withColumn("dummy",monotonicallyIncreasingId()+1)
		
					val table2 = Seq("A", "B").toDF("name")
					
					val numRows = table2.count()
					/*val reversedIds = (monotonicallyIncreasingId()* -1 + numRows + 1)*/ // Decrease 
					val reversedIds = (monotonicallyIncreasingId()+1)
					val dfWithIds = table2.withColumn("dummy", reversedIds)

					table1.show()			
					dfWithIds.show()

					

					val joinedDF = table1.join(dfWithIds, Seq("dummy"), "left_outer").drop("dummy")
					/*.select("id","name")
					.orderBy("id")
*/
					joinedDF.show()



/*
val joinedDF = table1.join(table2, Seq("dummy"), "left_outer").drop("dummy")
.select("id","name")
.orderBy("id")

joinedDF.show()*/

/*For example, if the DataFrame has 3 rows, then numRows will be 3. 
The expression monotonicallyIncreasingId() * -1 will generate IDs [-1, -2, -3].
Adding numRows + 1 to these IDs will result in [3, 2, 1], which is the desired range
of IDs for a monotonically decreasing sequence.*/
	}
}
===================================================================================================================
object sparkobj6 {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

import org.apache.spark.sql.functions

// create a sample DataFrame
val data = Seq(
  (101, "John", 50000),
  (101, "Mary", 60000),
  (102, "Bob", 70000),
  (102, "Alice", 80000),
  (103, "Joe", 90000),
  (103, "Sara", 100000)
)
val df = data.toDF("deptid", "name", "salary")

// group by deptid, find the maximum salary for each group, select deptid and max_salary columns,
// order by deptid, keep only the top 2 groups with highest max_salary, order by deptid and max_salary in descending order,
// drop the max_salary column

df.show()

val resultDF = df.groupBy("deptid") // maximum salary from each department
                  .agg(functions.max("salary").as("Maximium salary-Department wise"))
                   .orderBy("deptid")
                  
                 /* .orderBy(functions.desc("Maximium salary-Department wise"))*/ // from decending 

/*df.groupBy("deptid"): This groups the DataFrame df by the deptid column. 
 This means that rows with the same deptid value are grouped together.

.agg(functions.max("salary").as("max_salary")): 
This calculates the maximum value of the salary column for each group of rows with
 the same deptid value. 
 The resulting DataFrame will have two columns: deptid and max_salary. 
 The as method renames the max(salary) column to max_salary.

.orderBy(functions.desc("max_salary")): 
This orders the resulting DataFrame by the max_salary column in descending order. 
This means that the departments with the highest maximum salaries will appear
 first in the DataFrame.

So the final result of this code will be a DataFrame that shows the maximum salary for each department,
 ordered from highest to lowest maximum salary.


*/



/*val resultDF = df.groupBy("deptid") // lowest salary from each department
                  .agg(functions.min("salary").as("Minimum salary-Department wise"))
                  .orderBy("deptid")
*/


// show the resulting DataFrame
resultDF.show()

	}
}
===================================================================================================================
object sparkobj7 { // SPARK SQL OPERATIONS

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					import org.apache.spark.sql.functions
					
					
					
					val df = spark
					         .read
					         .option("header","true")
					         .csv("file:///C:/Users/manthena.r/GITHUB/Dataengineering/SQL/df.csv")
					val df1 = spark
					          .read
					          .option("header","true")
					          .csv("file:///c:/Users/manthena.r/GITHUB/Dataengineering/SQL/df1.csv")
					val cust = spark
					            .read
					            .option("header","true")
					          .csv("file:///c:/Users/manthena.r/GITHUB/Dataengineering/SQL/cust.csv")
					          
				 val prod = spark
				            .read
				            .option("header","true")
				            .csv("file:///c:/Users/manthena.r/GITHUB/Dataengineering/SQL/prod.csv")
				            
			              
				/*	df.show()
					df1.show()
					cust.show()
					prod.show()*/
					
					
					df.createOrReplaceTempView("df")
					df1.createOrReplaceTempView("df1")
					cust.createOrReplaceTempView("cust")
					prod.createOrReplaceTempView("prod")
					
					
		
		     /* spark.sql("select *from df") .show() 			// validate data
					spark.sql("select id,tdate from df order by id").show()  // select two columns
					spark.sql(" select id,tdate,category from df where category =  'Exercise' order by id").show()// Select column with category filter = Exercise
					spark.sql("select id, tdate,category,spendby from df where category = 'Exercise' and spendby = 'cash' ").show()// Multi Column filter
					spark.sql("select * from df where category in ('Exercise','Gymnastics')").show()//Multi Value Filter
			    spark.sql("select * from df where product like('%Gymnastics%')").show() //Like Filter
          spark.sql("select * from df where category != 'Exercise'").show() //Not Filters
          spark.sql(" select * from df where category not in ('Exercise','Gymnastics')").show()// Not In Filters
					spark.sql("select * from df where product is null").show() //Null Filters
			    spark.sql("select max(id) from df").show() //Max Function
          spark.sql(" select min(id) from df ").show() //Min Funtion
					spark.sql("select count(1) from df ").show() //Count 
					spark.sql("select * , case when spendby = 'cash' then 1 else 0 end as status from df ").show()  // Condition statement
					spark.sql("select id, category,concat(id,'-',category) as condata from df").show() // Concat data
					spark.sql("select concat_ws('-',id,category,product) as concat   from df    ").show()
				  spark.sql("select category,lower(category) as lower from df ").show() //Lower Case data
					spark.sql(" select amount,ceil(amount) from df").show()  //Ceil data
					spark.sql("select amount ,round(amount) as round from df ").show() // Round the data
					spark.sql("select coalesce(product,'NA') from df").show()  // Replace Nulls
					spark.sql("select trim(product) from df ").show() //Trim the space
					spark.sql(" select distinct category,spendby,amount from df").show // Distinct the columns
				  spark.sql("select substring(trim(product),1,10) from df").show() // Substring with Trim
				 	spark.sql("select SUBSTRING_INDEX(category,' ',1) as spl from df").show()  //Substring/Split operation
					spark.sql(" select * from df union all select * from df1 ").show()  // Union all
					spark.sql(" select * from df union select * from df1 order by id ").show() // Union
					spark.sql(" select category,sum(amount) as total  from df group by category").show()  //Aggregate Sum
					spark.sql(" select category,spendby,sum(amount) as total from df group by category,spendby").show()  // Aggregate sum with two columns
					spark.sql("select category,spendby,sum(amount) As total,count(amount)  as cnt from df group by category,spendby").show() //Aggregate Count
							
					spark.sql("select category, max(amount) as max from df group by category").show() // Aggregate Max
					
					spark.sql(" select category,max(amount) as max from df group by category order by category desc").show() // Aggregate with Order Descending 
	
					spark.sql(" select * , row_number() OVER(partition by category  order by amount asc) as rank from df").show() // Window Row Number
					
					
          spark.sql(" select * ,dense_rank() OVER(order by category asc) as Drank from df").show() //Window Dense_rank Number	
          spark.sql(" select * ,rank() OVER(order by category asc) as rank from df").show() //Window rank 	
						
					spark.sql("select * ,lead(amount) OVER(order by category asc) as lead from df").show()  //Window Lead function
	        spark.sql("select * ,lag(amount) OVER(order by category asc) as lag from df").show()  //Window Lag function
	        
					
					
			
				//	select city,count(*) as count from ranktable group by city having count(*)>1;
					// select first_name,last_name,city,count(*) as count from ranktable group by first_name,last_name,city having count(*)>1;
	        spark.sql("select category,count(category) as cnt from df group by category having count(category)>1").show()*/
	
	       /*spark.sql(" select * from cust join prod on cust.id = prod.id ").show() //Inner Join
	
	       spark.sql(" select * from cust left join prod on cust.id = prod.id").show() //Left Join
	       spark.sql("select a.id,a.name,b.product from cust a left join prod b on a.id=b.id").show()
	       
	       spark.sql("select * from cust right join  prod on cust.id = prod.id").show() //Right Join 
	       spark.sql("select a.id,a.name,b.product from cust a right join prod b on a.id=b.id").show()
	       spark.sql(" select * from cust full join prod on cust.id = prod.id").show() // Full Join
	       spark.sql("select a.id,a.name,b.product from cust a full join prod b on a.id=b.id").show()

					
				spark.sql("select * from cust LEFT ANTI JOIN prod on cust.id = prod.id").show() //left anti Join
				spark.sql("select a.id,a.name from cust a LEFT ANTI JOIN  prod b on a.id=b.id").show()
					
				spark.sql("select *,from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy'),'dd-MM-yyyy') as UPDATE from df").show()
			
				*/
					
							spark.sql("""

							select sum(amount) as total , con_date from(

							select id,tdate,from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy'),'MM-dd-yyyy') as con_date,amount,category,product,spendby from df) 

					    group by con_date

							""").show() // sub querry 
	}      
}
===================================================================================================================
object sparkobj8 { // Reading multiple csv files from folder

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					import org.apache.spark.sql.functions


    // Specify the folder containing the files to be concatenated
    val folderPath = ("file:///C:/data/april")

    // Read all files from the folder into a DataFrame
    val df = spark.read.option("header", "true").csv(folderPath + "/*.csv")

    
    df.coalesce(1).write.mode("overwrite").format("csv").save("file:///C:/data/forex.csv")
     println("complete")
     
   
}===================================================================================================================
}
package sparkpack

import org.apache.spark.SparkContext 
import org.apache.spark.sql.functions.concat
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql._ 
import org.apache.spark.sql.SaveMode
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.sql.functions.{col, to_date}
import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.io.Source
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.sql.types.DecimalType._
import org.apache.spark.sql.types.StructType
import org.apache.spark._
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming._
import org.apache.spark.sql._



object sparkobj  {
	def main(args: Array[String]): Unit = {
			println("=============== started ===============")
			val a = 2
			println(a)
			val b = "zeyo"
			println(b)

	}
}

object sparkobj1  {
	def main(args: Array[String]): Unit = {

			val proc = List("zeyo","zeyobron","zeyotron")
					val proc1 = List(2,4,6,8,10)


					println("****************PHASE-1 *******************")
					println("****************RAW LIST *******************")



					proc.foreach(println)

					/* val ls = proc.filter( x => x == ("zeyo") &&
                               x.contains("zeyobron"))
					 */
					val ls = proc.filter(x => x == "zeyo" | x == "zeyobron" | x == "rama")


					println("****************LIST CONTAINS ZEYO *******************")


					ls.foreach(println)

					val ps = proc.map( x => x.replace("zeyo","Rama"))
					println("****************REPLACING  ZEYO WITH RAMA *******************")

					ps.foreach(println)

					val cn = proc.map( x => x.concat(" Analytics"))

					println("****************CONCATING WITH ANALYTICS *******************")

					cn.foreach(println)

					println("****************PHASE-2 *******************")






					val xs = proc1.filter( x => x > 4 | x == 2 )
					println("**************** GREATER VALUES  *******************")
					xs.foreach(println)  
					println("****************THE-END ***********************************")


	}
}



object sparkobj2  { /* displaying zero first */
	def main(args: Array[String]): Unit = {

			val input = List(1,0,0,0,1,1)
					val zerofirst = input.partition(_== 0)
					val output = zerofirst._1 ::: zerofirst._2

					output.foreach(println)


	}
}
/*The given code is a Scala program that defines an object named "sparkobj2".
The object has a single method named "main" that takes an array of Strings 
as an input parameter. 
The main method defines a List named "input"
containing six integers, 
and then performs a partition operation
on the list based on the value of each element (0 or 1).


The partition method returns a tuple containing two lists: 

the first list contains all the elements that satisfy the given predicate
(in this case, the value is 0),
 and the second list contains all the elements that do not satisfy the given predicate (in this case, the value 
    is 1).

The program then concatenates these two lists using the ":::" operator
to create a new list named "output". 
Finally, the program uses a foreach loop to print each element of the 
"output" list.
 */

object sparkobj3  { /* displaying zero first */
	def main(args: Array[String]): Unit = {

			val input = List(1, 0, 0, 0, 1, 1)
					val zeros = input.filter(_ == 0)
					val nonZeros = input.filter(_ != 0)
					val output = zeros ++ nonZeros
					output.foreach(println)

	}
}



object sparkobj4 {// to find out email doesnt have @ string 


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._
					val df = spark
					.read
					.format("csv")
					.option("header", "false")
					.option("delimiter", ",")
					.load("file:///c:/data/email.txt")
					.toDF("sno", "name", "email","phone no")

					println("-----------------DATA--------------")
					df.show()

					/*val filteredDF = df.filter(!$"email".contains("@"))
					println("----------------- DATA MISSIN @ --------------")
					filteredDF.show()
					 */
					val filteredDF = df.filter(length(trim(col("phone no").cast(StringType))) < 10)


					println("----------------- DATA MISSIN @ --------------")
					filteredDF.show()




	}
}
object sparkobj5 {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					/*val table1 = Seq(1, 2, 3).toDF("id").withColumn("dummy",monotonicallyIncreasingId()+1)
val table2 = Seq("A", "B").toDF("name").withColumn("dummy",monotonicallyIncreasingId())

table1.show()
table2.show()*/

					val table1 = Seq(1, 2, 3).toDF("id").withColumn("dummy",monotonicallyIncreasingId()+1)

					val table2 = Seq("A", "B").toDF("name")

					val numRows = table2.count()
					/*val reversedIds = (monotonicallyIncreasingId()* -1 + numRows + 1)*/ // Decrease 
					val reversedIds = (monotonicallyIncreasingId()+1)
					val dfWithIds = table2.withColumn("dummy", reversedIds)

					table1.show()			
					dfWithIds.show()



					val joinedDF = table1.join(dfWithIds, Seq("dummy"), "left_outer").drop("dummy")
					/*.select("id","name")
					.orderBy("id")
					 */
					joinedDF.show()



					/*
val joinedDF = table1.join(table2, Seq("dummy"), "left_outer").drop("dummy")
.select("id","name")
.orderBy("id")

joinedDF.show()*/

					/*For example, if the DataFrame has 3 rows, then numRows will be 3. 
The expression monotonicallyIncreasingId() * -1 will generate IDs [-1, -2, -3].
Adding numRows + 1 to these IDs will result in [3, 2, 1], which is the desired range
of IDs for a monotonically decreasing sequence.*/
	}
}

object sparkobj6 {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					import org.apache.spark.sql.functions

					// create a sample DataFrame
					val data = Seq(
							(101, "John", 50000),
							(101, "Mary", 60000),
							(102, "Bob", 70000),
							(102, "Alice", 80000),
							(103, "Joe", 90000),
							(103, "Sara", 100000)
							)
					val df = data.toDF("deptid", "name", "salary")

					// group by deptid, find the maximum salary for each group, select deptid and max_salary columns,
					// order by deptid, keep only the top 2 groups with highest max_salary, order by deptid and max_salary in descending order,
					// drop the max_salary column

					df.show()

					/*val resultDF = df.groupBy("deptid") // maximum salary from each department
                  .agg(functions.max("salary").as("Maximium salary-Department wise"))
                   .orderBy("deptid")*/

					/* .orderBy(functions.desc("Maximium salary-Department wise"))*/ // from decending 

					/*df.groupBy("deptid"): This groups the DataFrame df by the deptid column. 
 This means that rows with the same deptid value are grouped together.

.agg(functions.max("salary").as("max_salary")): 
This calculates the maximum value of the salary column for each group of rows with
 the same deptid value. 
 The resulting DataFrame will have two columns: deptid and max_salary. 
 The as method renames the max(salary) column to max_salary.

.orderBy(functions.desc("max_salary")): 
This orders the resulting DataFrame by the max_salary column in descending order. 
This means that the departments with the highest maximum salaries will appear
 first in the DataFrame.

So the final result of this code will be a DataFrame that shows the maximum salary for each department,
 ordered from highest to lowest maximum salary.


					 */



					/*val resultDF = df.groupBy("deptid") // lowest salary from each department
                  .agg(functions.min("salary").as("Minimum salary-Department wise"))
                  .orderBy("deptid")
					 */

					val resultDF = df.groupBy("deptid") // average salary from each department
					.agg(functions.avg("salary").as("avg salary Department wise"))
					.orderBy("deptid")



					// show the resulting DataFrame
					resultDF.show()

	}
}


object sparkobj7 { // SPARK SQL OPERATIONS

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					import org.apache.spark.sql.functions



					val df = spark
					.read
					.option("header","true")
					.csv("file:///C:/Users/manthena.r/GITHUB/Dataengineering/SQL/df.csv")
					val df1 = spark
					.read
					.option("header","true")
					.csv("file:///c:/Users/manthena.r/GITHUB/Dataengineering/SQL/df1.csv")
					val cust = spark
					.read
					.option("header","true")
					.csv("file:///c:/Users/manthena.r/GITHUB/Dataengineering/SQL/cust.csv")

					val prod = spark
					.read
					.option("header","true")
					.csv("file:///c:/Users/manthena.r/GITHUB/Dataengineering/SQL/prod.csv")


					/*	df.show()
					df1.show()
					cust.show()
					prod.show()*/


					df.createOrReplaceTempView("df")
					df1.createOrReplaceTempView("df1")
					cust.createOrReplaceTempView("cust")
					prod.createOrReplaceTempView("prod")



					/* spark.sql("select *from df") .show() 			// validate data
					spark.sql("select id,tdate from df order by id").show()  // select two columns
					spark.sql(" select id,tdate,category from df where category =  'Exercise' order by id").show()// Select column with category filter = Exercise
					spark.sql("select id, tdate,category,spendby from df where category = 'Exercise' and spendby = 'cash' ").show()// Multi Column filter
					spark.sql("select * from df where category in ('Exercise','Gymnastics')").show()//Multi Value Filter
			    spark.sql("select * from df where product like('%Gymnastics%')").show() //Like Filter
          spark.sql("select * from df where category != 'Exercise'").show() //Not Filters
          spark.sql(" select * from df where category not in ('Exercise','Gymnastics')").show()// Not In Filters
					spark.sql("select * from df where product is null").show() //Null Filters
			    spark.sql("select max(id) from df").show() //Max Function
          spark.sql(" select min(id) from df ").show() //Min Funtion
					spark.sql("select count(1) from df ").show() //Count 
					spark.sql("select * , case when spendby = 'cash' then 1 else 0 end as status from df ").show()  // Condition statement
					spark.sql("select id, category,concat(id,'-',category) as condata from df").show() // Concat data
					spark.sql("select concat_ws('-',id,category,product) as concat   from df    ").show()
				  spark.sql("select category,lower(category) as lower from df ").show() //Lower Case data
					spark.sql(" select amount,ceil(amount) from df").show()  //Ceil data
					spark.sql("select amount ,round(amount) as round from df ").show() // Round the data
					spark.sql("select coalesce(product,'NA') from df").show()  // Replace Nulls
					spark.sql("select trim(product) from df ").show() //Trim the space
					spark.sql(" select distinct category,spendby,amount from df").show // Distinct the columns
				  spark.sql("select substring(trim(product),1,10) from df").show() // Substring with Trim
				 	spark.sql("select SUBSTRING_INDEX(category,' ',1) as spl from df").show()  //Substring/Split operation
					spark.sql(" select * from df union all select * from df1 ").show()  // Union all
					spark.sql(" select * from df union select * from df1 order by id ").show() // Union
					spark.sql(" select category,sum(amount) as total  from df group by category").show()  //Aggregate Sum
					spark.sql(" select category,spendby,sum(amount) as total from df group by category,spendby").show()  // Aggregate sum with two columns
					spark.sql("select category,spendby,sum(amount) As total,count(amount)  as cnt from df group by category,spendby").show() //Aggregate Count

					spark.sql("select category, max(amount) as max from df group by category").show() // Aggregate Max

					spark.sql(" select category,max(amount) as max from df group by category order by category desc").show() // Aggregate with Order Descending 

					spark.sql(" select * , row_number() OVER(partition by category  order by amount asc) as rank from df").show() // Window Row Number


          spark.sql(" select * ,dense_rank() OVER(order by category asc) as Drank from df").show() //Window Dense_rank Number	
          spark.sql(" select * ,rank() OVER(order by category asc) as rank from df").show() //Window rank 	

					spark.sql("select * ,lead(amount) OVER(order by category asc) as lead from df").show()  //Window Lead function
	        spark.sql("select * ,lag(amount) OVER(order by category asc) as lag from df").show()  //Window Lag function




				//	select city,count(*) as count from ranktable group by city having count(*)>1;
					// select first_name,last_name,city,count(*) as count from ranktable group by first_name,last_name,city having count(*)>1;
	        spark.sql("select category,count(category) as cnt from df group by category having count(category)>1").show()*/

					/*spark.sql(" select * from cust join prod on cust.id = prod.id ").show() //Inner Join

	       spark.sql(" select * from cust left join prod on cust.id = prod.id").show() //Left Join
	       spark.sql("select a.id,a.name,b.product from cust a left join prod b on a.id=b.id").show()

	       spark.sql("select * from cust right join  prod on cust.id = prod.id").show() //Right Join 
	       spark.sql("select a.id,a.name,b.product from cust a right join prod b on a.id=b.id").show()
	       spark.sql(" select * from cust full join prod on cust.id = prod.id").show() // Full Join
	       spark.sql("select a.id,a.name,b.product from cust a full join prod b on a.id=b.id").show()


				spark.sql("select * from cust LEFT ANTI JOIN prod on cust.id = prod.id").show() //left anti Join
				spark.sql("select a.id,a.name from cust a LEFT ANTI JOIN  prod b on a.id=b.id").show()

				spark.sql("select *,from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy'),'dd-MM-yyyy') as UPDATE from df").show()

					 */

					spark.sql("""

							select sum(amount) as total , con_date from(

							select id,tdate,from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy'),'MM-dd-yyyy') as con_date,amount,category,product,spendby from df) 

							group by con_date

							""").show() // sub querry 
	}      
}

object sparkobj8 { // SPARK SQL OPERATIONS

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					import org.apache.spark.sql.functions


					// Specify the folder containing the files to be concatenated
					val folderPath = ("file:///C:/data/april")

					// Read all files from the folder into a DataFrame
					val df = spark.read.option("header", "true").csv(folderPath + "/*.csv")
					df.show(20)

					df.createOrReplaceTempView("df")




					/*val df1 = spark.sql("select  *  from df where SERIES = 'EQ'order by  TIMESTAMP ")
					
							spark.sql(" select id,tdate,category from df where category =  'Exercise' order by id").show()// Select column with category filter = Exercise
					spark.sql("select id, tdate,category,spendby from df where category = 'Exercise' and spendby = 'cash' ").show()// Multi Column filter


    df1.coalesce(1).write.mode("overwrite").format("csv").save("file:///C:/data/EQ.csv")
     println("complete")

    Concatenate all columns into a single column using the concat function
    val concatenatedDF = df.select(concat(df.columns.map(df(_)): _*).as("concatenated"))

    concatenatedDF.show()*/
	}

}



object sparkobj9 { // SPARK SQL OPERATIONS

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate() // DATA FRAME PROPERTIES
					import spark.implicits._

					import org.apache.spark.sql.functions


					val df = spark
					         .read
					          .format("com.databricks.spark.csv")
					         .option("header","true")
					         	.option("delimiter", ",")
					         .load("file:///C:/data/aprileq.csv")
					         .toDF("Company","SERIES","OPEN","HIGH","LOW","CLOSE","LAST","PREVCLOSE","TOTTRDQTY","TOTTRDVAL","DATE","TT","ISIN","Null")
					    /*     df.show()*/
	    
					         
					         df.createOrReplaceTempView("df")
					         
					      spark.sql("SELECT * from df  where Company ='5PAISA' order by HIGH Desc").show()


/*case class StockPrice(date: String, open: Float, high: Float, low: Float, close: Float, volume: Int)

val filename = "path/to/your/file.csv"
val folderPath = ("file:///C:/data/dataapril")
val stockPrices = Source.fromFile(filename)
  .getLines()
  .drop(1) // skip header row
  .map { line =>
    val fields = line.split(",")
    StockPrice(
      date = fields(0),
      open = fields(1).toFloat,
      high = fields(2).toFloat,
      low = fields(3).toFloat,
      close = fields(4).toFloat,
      volume = fields(5).toInt
    )
  }
  .toList

val spark = SparkSession.builder().appName("StockPricePrediction").getOrCreate()

// Convert list of StockPrice objects to DataFrame
val df = stockPrices
  .toDF()
  .withColumn("date", to_date(col("date"), "yyyy-MM-dd")) // convert date string to DateType

// Split data into training and test sets
val Array(trainingData, testData) = df.randomSplit(Array(0.7, 0.3))

// Calculate average closing price
val avgClosingPrice = stockPrices.map(_.close).sum / stockPrices.length
println(s"Average closing price: $avgClosingPrice")

// Define feature vector assembler
val assembler = new VectorAssembler()
  .setInputCols(Array("open", "high", "low", "volume"))
  .setOutputCol("features")

// Transform input data into feature vectors
val trainingFeatures = assembler.transform(trainingData)
val testFeatures = assembler.transform(testData)

// Define linear regression model
val lr = new LinearRegression()
  .setLabelCol("close")
  .setFeaturesCol("features")

// Train model on training data
val model = lr.fit(trainingFeatures)

// Make predictions on test data
val predictions = model.transform(testFeatures)

// Print predicted closing prices for the first 10 days of the test period
predictions.select("date", "close", "prediction").show(10)*/
					
	}

}




object sparkobj10 { // Streaming 


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]").set("spark.driver.allowMultipleContexts","true")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")
					val spark = SparkSession
					.builder()

					.getOrCreate()
					import spark.implicits._


					val ssc = new StreamingContext(conf,Seconds(1))


					val stream = ssc.textFileStream("file:///C:/nout")  


					stream.print()




					ssc.start()
					ssc.awaitTermination()




	}



}




=================================================================================================
  import org.apache.spark.SparkContext 
import org.apache.spark.sql.functions.concat
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql._ 
import org.apache.spark.sql.SaveMode
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.sql.functions.{col, to_date}
import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.io.Source
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.sql.types.DecimalType._
import org.apache.spark.sql.types.StructType
import org.apache.spark._
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming._
import org.apache.spark.sql._
================================================================================================



